//#define NDEBUG
#include"news_process.h"
#include"string_fun.h"
#include"pcre.h"
#include"pcrecpp.h"
#include<stdio.h>
#include<pthread.h>
#include "glog/logging.h"
void print_page_struct(struct page_info* p_temp_page);
void print_page_array(vector<struct page_info*>& page_array);
//vector排序函数
bool sort_by_hot_rank(struct page_info* v1, struct page_info* v2)
{
	if( (v1->hot_rank - v2->hot_rank < 0.000001) && (v1->hot_rank - v2->hot_rank > -0.000001) && v1->site_rank == v2->site_rank)
	{
		return v1->newsrank > v2->newsrank;
	}
	else if( (v1->hot_rank - v2->hot_rank < 0.000001) && (v1->hot_rank - v2->hot_rank > -0.000001))
	{
		return v1->site_rank > v2->site_rank;
	}
	else
	{
		return v1->hot_rank > v2->hot_rank;
	}
}

int NewsProcess::init(const string& file_name)
{

	this->file_name = file_name;
	this->classification_name = StringFun::get_file_class(file_name);
	this->file_generated_time = StringFun::get_file_generated_time(file_name);
	this->file_generated_time_str = StringFun::get_file_generated_time_str(file_name);

	if(0 == this->classification_name.length())	
	{
		DLOG(ERROR) <<"classification_name.length() = 0, file_name = " << this->file_name;
		return -1;
	}

	if(open_outfile_handle() != 0)
	{
		return -1;
	}

	//按发帖所处的小时数加权
	initialize_hour_weighted_page_nums();

	int ret1 = read_config(CONFIG_DIR + "/safe_sites.txt", this->safe_sites);	
	int ret14 = read_config(CONFIG_DIR + "/black_sites.txt", this->black_sites);	

	int ret2 = read_config(CONFIG_DIR + "/black_keyword.txt", this->black_keywords);	

	int ret3 = read_config(CONFIG_DIR + "/sites_highlighted.txt", this->sites_highlighted);	
	int ret4 = read_config(CONFIG_DIR + "/sites_official.txt", this->sites_official);	
	int ret5 = read_config(CONFIG_DIR + "/hot_sites_it.txt", this->hot_sites_it);	
	int ret6 = read_config(CONFIG_DIR + "/hot_sites_science.txt", this->hot_sites_science);	
	int ret7 = read_config(CONFIG_DIR + "/hot_sites_car.txt", this->hot_sites_car);	
	int ret8 = read_config(CONFIG_DIR + "/hot_sites_economy.txt", this->hot_sites_economy);	
	int ret9 = read_config(CONFIG_DIR + "/hot_sites_militery.txt", this->hot_sites_militery);	
	int ret10 = read_config(CONFIG_DIR +"/hot_sites_women.txt", this->hot_sites_women);	

	int ret11 = read_config(CONFIG_DIR + "/title_filted_chars.txt",this->filted_chars_in_title);

	int ret12 = read_config(CONFIG_DIR + "/hot_words_it.txt",this->hot_words_it);
	int ret13 = read_config(CONFIG_DIR + "/hot_words_science.txt",this->hot_words_science);

	if((ret1|ret2|ret3|ret4|ret5|ret6|ret7|ret8|ret9|ret10|ret11|ret12|ret13|ret14) != 0)
	{
		return -1;
	}

	return 0;

}
int NewsProcess::make_dir(const string& dir_name)
{
	DIR *p_dir;
	if((p_dir = opendir(dir_name.c_str())) == NULL)
	{
		if(mkdir(dir_name.c_str(),S_IRWXU) != 0)
		{
			DLOG(ERROR) <<  "can't mkdir  " + dir_name;
			return -1;
		}
	}
	closedir(p_dir);
	return 0;
}
int NewsProcess::open_outfile_handle()
{


	
	string classification_out_dir = OUTPUT_DIR + "/" + this->classification_name;
	
	int ret1 = make_dir(classification_out_dir);
	int ret2 = make_dir(META_DIR);
	int ret3 = make_dir(NEWS_HOT_DIR);
	int ret4 = make_dir(HOT_RANK_DIR);	

	if((ret1 | ret2 | ret3 | ret4) != 0 )
	{
		return -1;
	}	

	this->outfile_name = classification_out_dir +  "/" + this->classification_name + "_" + this->file_generated_time_str + ".txt" ;

	string temp_outfile_name = this->outfile_name + ".bak";
	this->outfile_handle.open(temp_outfile_name.c_str(),ofstream::out);

	if(!this->outfile_handle)
	{
		DLOG(ERROR) <<  "can't open  " + temp_outfile_name;
		return -1;
	}

	//meta文件
	string meta_filename = META_DIR + "/meta_" + this->classification_name + "_" +  this->file_generated_time_str + ".txt";
	this->meta_handle.open(meta_filename.c_str(), ofstream::out);
	if(!this->meta_handle)
	{
		DLOG(ERROR) <<  "can't open  " + meta_filename;
		return -1;
	}

	if("domestic" == this->classification_name || "social" == this->classification_name || "international" == this->classification_name || "economy" == this->classification_name)
	{
		//hot_rank文件
		string hotrank_dir = HOT_RANK_DIR + "/" + this->classification_name;
		if(make_dir(hotrank_dir) != 0)
		{
			return -1;
		}			

		this->hotrank_filename = hotrank_dir + "/" + this->classification_name + "_" + this->file_generated_time_str + ".data";
		string temp_hotrank_filename = this->hotrank_filename + ".bak";	
		this->hotrank_handle.open(temp_hotrank_filename.c_str(), ofstream::out);
		if(!this->hotrank_handle)
		{
			DLOG(ERROR) <<  "can't open  " + hotrank_filename;
			return -1;
		}	
	}

	//news_hot文件
	string day_time = this->file_generated_time_str.substr(0, 10);	
	string newshot_filename = NEWS_HOT_DIR + "/hotnews."  + day_time;

	this->newshot_handle.open(newshot_filename.c_str(),ofstream::out|ofstream::app);
	if(!this->newshot_handle)
	{
		DLOG(ERROR) <<  "can't open  " + newshot_filename;
		return -1;
	}

	return 0;
}

int NewsProcess::read_config(const string& config_filename, vector<string>& vec_name)
{
	if(0 == file_name.length())
	{
		return -1;
	}
	std::ifstream infile(config_filename.c_str());
	if(!infile)
	{
		LOG(ERROR) << "can't open " <<  config_filename ;
		return -1;
	}
	string line = "";
	while(getline(infile, line))
	{
		if(line != "")
		{
			vec_name.push_back(line);	
		}	
	}
	infile.close();
	return 0;
}

int NewsProcess::read_config(const string& config_filename, set<string>& set_name)
{
	if(0 == file_name.length())
	{
		return -1;
	}
	std::ifstream infile(config_filename.c_str());
	if(!infile)
	{
		LOG(ERROR) << "can't open " << config_filename;
		return -1;
	}
	string line = "";
	while(getline(infile, line))
	{
		if(line != "")
		{
			set_name.insert(line);
		}		
	}
	infile.close();
	return 0;
}
int NewsProcess::read_config(const string& config_filename, map<string, double>& map_name)
{
	if(0 == file_name.length())
	{
		return -1;
	}
	std::ifstream infile(config_filename.c_str());
	if(!infile)
	{
		LOG(ERROR) << "can't open: " << config_filename;
		return -1;
	}
	string line = "";
	string key = "";
	string value = "";
	while(getline(infile, line))
	{
		std::istringstream stream(line);
		stream >> key >> value;
		if(key != "" && value != "")
		{
			double value_double = atof(value.c_str());

			// the keys value can be 0.0  which means forbidden the key(the key can be a site or a keyword)
			map_name[key] = value_double;
		}		
	}
	infile.close();
	return 0;
}

void NewsProcess::initialize_hour_weighted_page_nums()
{

	//根据发帖时间pdate，对发帖次数加权 。 比如 如果某个帖子是在0点发布，则默认发一篇帖子等于是发了6篇
	this->hour_weighted_page_nums.push_back(6); // 0-> 6
	this->hour_weighted_page_nums.push_back(6); // 1-> 6
	this->hour_weighted_page_nums.push_back(6); // 2-> 6 
	this->hour_weighted_page_nums.push_back(6); // ..... 
	this->hour_weighted_page_nums.push_back(6); // 23->6
	this->hour_weighted_page_nums.push_back(5);
	this->hour_weighted_page_nums.push_back(4);
	this->hour_weighted_page_nums.push_back(2);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(2);
	this->hour_weighted_page_nums.push_back(2);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(1);
	this->hour_weighted_page_nums.push_back(4);
	this->hour_weighted_page_nums.push_back(4);
	this->hour_weighted_page_nums.push_back(6);
	this->hour_weighted_page_nums.push_back(6);
	this->hour_weighted_page_nums.push_back(6);
	this->hour_weighted_page_nums.push_back(6);

}

//对每一篇文章，统计出发帖所处的时间段,并对数量加权 
//统计时参考时间为 referenced_time
int NewsProcess::statistic_section(const time_t& referenced_time, struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}
	struct tm *p_tm;
	time_t current_time = referenced_time;
	p_tm = localtime(&current_time);

	int day_weight = 1; 
	int day = p_tm->tm_mday;
	if(0 == day || 6 == day) // 周六以及周日发帖加权 
	{
		day_weight = 4;
	}

	time_t publish_date = p_page_info->pdate;
	time_t crawl_date = p_page_info->crawltime;
	time_t final_date = publish_date;


	if(crawl_date - publish_date < 86400 && crawl_date - publish_date > 900)
	{
		final_date = crawl_date;
	}


	p_tm = localtime(&final_date);
	int hour = p_tm->tm_hour;
	int hour_weighted_nums = this->hour_weighted_page_nums[hour];	

	double time_diff = current_time - final_date;

	//cout << "publish_date: " << publish_date << endl;
	//cout << "crawl_date: " << crawl_date << endl;
	//cout <<"final_date: " << final_date << endl;
	//cout << "time_diff: " << time_diff << endl;

	//统计发帖时间所处分段(参考时间为当前时间)
	if(time_diff > 24*3600 || time_diff < 0)
	{
		//cout << "time_filter: " << p_page_info->url << "\t" << p_page_info->pdate << "\t" << time_diff << "\n";
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "time_diff\t";
		outline += p_page_info->url + "\t" ;
		outline += p_page_info->title + "\t" ;
		DLOG(INFO) << outline;
#endif
		return -1;
	}
	else if(time_diff >= 16*3600)
	{
		p_page_info-> section = section_4;
	}
	else if(time_diff >= 12*3600)
	{
		p_page_info-> section = section_3;
	}
	else if(time_diff >= 3*3600)
	{
		p_page_info-> section = section_2;
	}
	else if(time_diff >= 1.5*3600)
	{
		p_page_info-> section = section_1;
	}
	else if(time_diff >= 0)
	{
		p_page_info-> section = section_0;
	}

	//计算加权发帖数
	p_page_info->section_weighted_nums = day_weight * hour_weighted_nums;		
	return 0;	
}

//TODO:ceshi
//初始化文章的site_rank值
int NewsProcess::initialize_site_rank(struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}
	string site = p_page_info->site;
	p_page_info->site_rank = 0;

	//比较敏感的几个分类需要评定下网站是否安全
	if("domestic" == this->classification_name || "social" == this->classification_name || "international" == this->classification_name || "economy" == this->classification_name)
	{
		if(safe_sites.count(site) != 0)
		{
			p_page_info->site_rank = 1;			
		}

	}
	//除了上述分类外，对于其他的分类，在首页中,重点显示this->sites_highlighted中的网页
	else
	{
		for(vector<string>::const_iterator iter = this->sites_highlighted.begin(); iter != this->sites_highlighted.end(); iter++)
		{
			if(site.find(*iter) != string::npos)
			{
				p_page_info->site_rank = 1;
				break;
			}
		}
	}	

	return 0;
}
int NewsProcess::initialize_site_factor(struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}

	p_page_info->site_factor = 1.0;
	string site = p_page_info->site;



	if("domestic" == this->classification_name || "social" == this->classification_name || "international" == this->classification_name || "economy" == this->classification_name)
	{
		if(p_page_info->newsrank > 85)
		{
			p_page_info->site_factor = 3.0;
		}
	}
	else if("it" == this->classification_name)
	{
		if(this->hot_sites_it.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_it[site];
		}
	}
	else if("science" == this->classification_name)
	{
		if(this->hot_sites_science.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_science[site];
		}
	}
	else if("car" == this->classification_name)
	{
		if(this->hot_sites_car.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_car[site];
		}
	}
	else if("economy" == this->classification_name)
	{
		if(this->hot_sites_economy.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_economy[site];
		}
	}

	else if("militery" == this->classification_name)
	{
		if(this->hot_sites_militery.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_militery[site];
		}
	}
	else if("women" == this->classification_name)
	{
		if(this->hot_sites_women.count(site) != 0)
		{
			p_page_info->site_factor = this->hot_sites_women[site];
		}
	}
	return 0;
}

//统计关键词落在每个时间段的个数以及含有此关键词的文章集合
int NewsProcess::statistics_all_keywords_details()
{
	for(vector<struct page_info*>::const_iterator vector_iter = this->page_array.begin() ; vector_iter != this->page_array.end(); vector_iter++)
	{

		double site_factor = (*vector_iter)->site_factor;

		string key_word("");
		for(int i = 0; i != MAX_KEYWORDS_NUMS; i++)
		{ // 遍历page中的关键词
			if("" != (*vector_iter)->keywords[i])
			{
				key_word = (*vector_iter)->keywords[i];


				// 初始化关键词权值
				double word_factor = 1.0;

				if(this->classification_name == "it")
				{
					if(this->hot_words_it.count(key_word) != 0)
					{
						word_factor = this->hot_words_it[key_word];
					}
				}
				else if(this->classification_name == "science")
				{
					if(this->hot_words_science.count(key_word) != 0)
					{
						word_factor = this->hot_words_science[key_word];
					}
				}
				


				map<string, struct keyword_info*>::iterator map_iter = this->key_map.find(key_word);
				if(map_iter != this->key_map.end()) // 关键词已经记录过
				{
					(map_iter->second)->pos_in_page_array->push_back((*vector_iter)->pos_in_array);
					int temp_frg = (*vector_iter)->section;
					(map_iter->second)->section_nums[temp_frg]+=( (*vector_iter)->section_weighted_nums * site_factor * word_factor );

#ifndef NDEBUG

					map_iter->second->section_nums_original[temp_frg] += 1;

#endif


				}
				else  // 关键词第一次出现
				{
					//为关键词结构体分配空间
					struct keyword_info *p_temp_keyword_info = NULL; 
					p_temp_keyword_info = new struct keyword_info();

					if(NULL == p_temp_keyword_info)
					{
						cerr << "new error" << endl;
						return -1 ;
					}
					//为保存关键词序号的结构分配空间
					p_temp_keyword_info->pos_in_page_array = new vector<unsigned long int>();
					if(NULL == p_temp_keyword_info->pos_in_page_array)
					{
						cerr << "new error " <<endl;
					}

					//记录含有此关键词的文档
					p_temp_keyword_info->pos_in_page_array->push_back((*vector_iter)->pos_in_array); 

					//分段累积
					int temp_frg = (*vector_iter)->section;
					p_temp_keyword_info->section_nums[temp_frg]+=( (*vector_iter)->section_weighted_nums * site_factor * word_factor);

#ifndef NDEBUG

					p_temp_keyword_info->section_nums_original[temp_frg] += 1;

#endif

					//将关键词加入到key_map中
					this->key_map[key_word] = p_temp_keyword_info; 



				} 
			}
		}
	}
	return 0;
}

//

//计算关键词的权值
void NewsProcess::calculate_key_weight()
{

	for(map<string, struct keyword_info*>::iterator iter = this->key_map.begin(); iter != this->key_map.end(); iter++)
	{
		// 每个分段下关键词出现的次数,比如 section_count[0]就表示关键词在第一个分段下出现的次数
		double section_count[SECTION_NUMS] = {0.0};
		double key_total_count = 0.0;
		double key_average_count =0.0;  // 关键词平均出现的次数
		double key_weight = 1.0;

#ifndef NDEBUG
		double key_total_count_original = 0.0;
#endif

		for(int i = 0; i < SECTION_NUMS; i++)
		{
			//这里将关键词权值减小，防止后续计算中关键词对应的权值太大而出现溢出。
			if(i == 3)
			{
				section_count[i] = iter->second->section_nums[i]/4.0;  
			}
			else if(i == 4)
			{
				section_count[i] = iter->second->section_nums[i]/8.0;  

			}
			else
			{
				section_count[i] = iter->second->section_nums[i]/3.0;  

			}
			key_total_count += section_count[i];

#ifndef NDEBUG
			key_total_count_original += iter->second->section_nums_original[i];
#endif
		} 

#ifndef NDEBUG
		iter->second->key_nums_original = key_total_count_original;
#endif

		key_average_count = key_total_count / SECTION_NUMS;

		iter->second->key_avg_nums = key_average_count;
		//
		if(key_average_count > 100)
		{
			key_weight = 3;
		}
		else if(section_count[0] >= 5 * key_average_count && section_count[0] >= 4)
		{
			key_weight = 60;
		}
		else if(section_count[0] >=3.5 * key_average_count && section_count[0] >= 4)
		{
			key_weight = 20;
		}
		else if(section_count[0] > 2 * key_average_count)
		{
			key_weight = 7;
		}
		else if(section_count[0] > key_average_count)
		{
			key_weight = 6;
		}
		else if(section_count[SECTION_NUMS-1] > 2 * key_average_count)
		{
			key_weight = 0.2;	
		}
		else if(section_count[SECTION_NUMS-1] + section_count[SECTION_NUMS-2] > key_average_count)
		{
			key_weight = 0.3;
		}
		else if(section_count[0] + section_count[1] > key_average_count)
		{
			key_weight = 5;
		}

		if(key_weight > 6 && (section_count[0] / (section_count[1] + 1)) > 2)
		{
			key_weight *= 2;
		}

		// 关键词最后的权值
		iter->second->key_value = key_weight * key_average_count;
	}
}

//过滤不符合条件的page
bool NewsProcess::is_filted_page(const time_t &referenced_time,const struct page_info* p_page_info)
{
	//过滤没有pdate的新闻
	struct tm *p_tm;
	time_t publish_date = p_page_info->pdate;
	p_tm = localtime(&publish_date);
	if(0 == p_tm->tm_hour && 0 == p_tm->tm_min && 0 == p_tm->tm_sec )
		return true;

	int pdate_day = p_tm->tm_mday;
	time_t current_time = referenced_time ;
	p_tm = localtime(&current_time);
	int current_day = p_tm->tm_mday;

	//只选择今天的新闻
	if(p_tm->tm_hour >=7 && pdate_day != current_day)
	{
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "pdate\t";
		outline += p_page_info->url + "\t" ;
		outline += p_page_info->title + "\t" ;

		DLOG(INFO) << outline; 
#endif

		return true;
	}

	//只选择官方新闻
	if(("domestic" == this->classification_name || "social" == this->classification_name || "international" == this->classification_name)
			&& (p_page_info->newsrank < 81) && (0  == p_page_info->site_rank) )
	{
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "site_news_rank\t";
		outline += p_page_info->url + "\t" ;
		outline += p_page_info->title + "\t" ;

		DLOG(INFO) << outline; 
#endif
		return true;
	}

	//如果标题中含有两个以上的空格，则舍弃
	string temp_title = p_page_info->title;
	size_t length_before = temp_title.length();
	StringFun::replace_all(temp_title, " ", "");
	size_t length_end = temp_title.length();
	if(length_before - length_end >= 2)
	{
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "title\t";
		outline += p_page_info->url + "\t" ;
		outline += p_page_info->title + "\t" ;

		DLOG(INFO) << outline; 
#endif
		return true;
	}

//过滤含有特殊字符的标题
for(vector<string>::const_iterator iter = this->filted_chars_in_title.begin(); iter != this->filted_chars_in_title.end(); iter++)
{
	if((p_page_info->title).find(*iter) != string::npos)
	{
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "title\t";
		outline += p_page_info->url + "\t" ;
		outline += p_page_info->title + "\t" ;

		DLOG(INFO) << outline; 
#endif
		return true;
	}

}

return false;


}
int NewsProcess::record_page_with_same_keywords(set<int>& exsited_key_pos, const struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}
	struct keyword_info *temp_keyword_info = NULL;                                                     
	string keywords_join = "";        //将所有关键词拼接，便于输出
	string keywords_value_join = "";  
	string out_line = "";
	string keywords = "";  

	//	cout << endl << endl;                                                 
	for(int i = 0; i != MAX_KEYWORDS_NUMS; i++)                                                           
	{                                                                                                     
		keywords = p_page_info->keywords[i];  //依次提取出p_page_info中的关键词，并将所有包含此关键词的文章记录进exsited_key_pos                                                       
		if("" != keywords)                                                                            
		{                                  
			keywords_join += " " + keywords;
			keywords_value_join += " " + StringFun::to_string(this->key_map[keywords]->key_value);

			temp_keyword_info = key_map[keywords];                                                
			for(vector<unsigned long int>::iterator key_iter = temp_keyword_info->pos_in_page_array->begin();        
					key_iter != temp_keyword_info->pos_in_page_array->end(); key_iter++ )                 
			{
				exsited_key_pos.insert(*key_iter);
			}
		}
	}

	return 0;
}


int NewsProcess::output_result(const struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}
	struct keyword_info *temp_keyword_info = NULL;                                                     
	string keywords_join = "";        //将所有关键词拼接，便于输出
	string keywords_value_join = "";  
	string out_line = "";
	string keywords = "";  

	for(int i = 0; i != MAX_KEYWORDS_NUMS; i++)                                                           
	{                                                                                                     
		keywords = p_page_info->keywords[i];  //依次提取出p_page_info中的关键词，并将所有包含此关键词的文章记录进exsited_key_pos                                                       
		if("" != keywords)                                                                            
		{                                  
			keywords_join += " " + keywords;
			keywords_value_join += " " + StringFun::to_string(this->key_map[keywords]->key_value);

		}
	}

	//输出文章
	this->outfile_handle << (p_page_info->title + "\t" +  p_page_info->url + "\t") << p_page_info->pdate << "\t" << StringFun::to_string(p_page_info->hot_rank) << "\t" << (keywords_join + "\t" + keywords_value_join + "\n") ;
	this->newshot_handle << this->file_generated_time << "\t" << this->classification_name << "\t" << p_page_info->title << "\t" << p_page_info->url << "\t" << p_page_info->pdate << "\t" << p_page_info->hot_rank <<"\t" <<  keywords_join << "\n";		
	return 0;
}


bool NewsProcess::is_reach_the_threshold(map<string, int>& sites_count,const string &site, const int &threshold)
{

	for(map<string,int>::iterator iter = sites_count.begin(); iter != sites_count.end(); iter++)
	{
		if(site.find(iter->first) != string::npos)
		{
			if(sites_count[iter->first] >= threshold)
			{

				return true;
			}
			else
			{
				sites_count[iter->first]++;
				return false;
			}
		}
	}
	return false;

}
int  NewsProcess::output_meta_sites(ofstream& out_handle, struct page_info* p_page_info)
{
	if(NULL == p_page_info)
	{
		return -1;
	}
	string temp_site = p_page_info->site;
	for(vector<string>::iterator iter = sites_meta.begin(); iter != sites_meta.end(); iter++)
	{
		if(temp_site.find(*iter) != string::npos)
		{
			out_handle << p_page_info->title << "\t" << p_page_info->url << "\t" << p_page_info->pdate << "\n";
			break;
		}
	}
	return 0;
}
//top_n 表示输出前n个热点新闻
void NewsProcess::output_top_n_news(const int& top_n)
{



	//如果某个文章已经输出到文件中，假设此文章对应的标题分出的词为key1, key2, key3,则后续含有这三个关键词中任意一个关键
	//词的文章都会被过滤，exsited_key_pos 就是保存含有已输出关键词的文章的在page_array中的位置
	set<int> exsited_key_pos;


	vector<string> title_array;
	map<string, int> sites_count;
	int top_m = 20;
	int threshold = 3;


	if("domestic" == this->classification_name || "social" == this->classification_name || "international" == this->classification_name || "economy" == this->classification_name)
	{
		for(vector<string>::iterator official_iter = this->sites_official.begin(); official_iter != this->sites_official.end(); official_iter++)
		{
			sites_count[*official_iter] = 0;	
		}
	}
	else
	{	
		for(vector<string>::iterator highlighted_iter = this->sites_highlighted.begin(); highlighted_iter != this->sites_highlighted.end(); highlighted_iter++)
		{
			sites_count[*highlighted_iter] = 0;
		}
	}


	int output_count = top_n;

	for(vector<struct page_info*>::iterator page_iter = this->page_array.begin(); page_iter != this->page_array.end(); page_iter++)
	{
		//过滤掉不符合要求的新闻（比如标题含有特殊字符，标题过长，过短等等）
		if(is_filted_page(this->file_generated_time, *page_iter))
		{
			continue;
		}


		//输出传媒站点
//		output_meta_sites(meta_handle, *page_iter);


		//如果相同主题的文章已经输出到文件中，则忽略此文章
		if(exsited_key_pos.count((*page_iter)->pos_in_array) !=  0)
		{
#ifndef NDEBUG
			string outline = "filter_info\t";
			outline += this->classification_name + "\t";
			outline += this->file_generated_time_str + "\t"; 
			outline += "key_exist\t";
			outline += (*page_iter)->url + "\t" ;
			outline += (*page_iter)->title + "\t" ;

			DLOG(INFO) << outline; 
#endif


			//输出hotrank
			if(this->hotrank_handle)
			{
				int last_rank =	output_count + 1;
				this->hotrank_handle << (*page_iter)->url << "\t" << last_rank << "\n"; 
			}	

			continue;
		}

		if(title_array.size() < top_m )
		{
			//对top_m 个文章，控制sites_official和sites_highlighted中网站展示数目
			//如果展现数目达到阈值threshold，则过滤，否则，将此网站加入sites_count集合
			if(is_reach_the_threshold(sites_count,(*page_iter)->site, threshold))
			{
#ifndef NDEBUG
				string outline = "filter_info\t";
				outline += this->classification_name + "\t";
				outline += this->file_generated_time_str + "\t"; 
				outline += "sites_count\t";
				outline += (*page_iter)->url + "\t" ;
				outline += (*page_iter)->title + "\t" ;
				DLOG(INFO) << outline; 
#endif
				continue;
			}

			//对前top_m个标题去重，连续两个字相同则忽略
			bool skip = false;
			for(vector<string>::iterator title_iter = title_array.begin(); title_iter !=title_array.end(); title_iter++)
			{
				int same_count = StringFun::get_LCS(*title_iter, (*page_iter)->title);
				if(same_count >= 2)
				{
#ifndef NDEBUG
					string outline = "filter_info\t";
					outline += this->classification_name + "\t";
					outline += StringFun::to_string(this->file_generated_time) + "\t"; 
					outline += "LCS\t";
					outline += (*page_iter)->url + "\t" ;
					outline += (*page_iter)->title + "\t" ;
					outline += "similar_title:\t" + *title_iter; 
					DLOG(INFO) << outline; 
#endif
					skip = true;
					break;
				} 
			}
			if(true == skip)
			{
				continue;
			}

			title_array.push_back((*page_iter)->title);

		}


		//输出此文章，并将与此文章含有相同关键词的所有文章记录到i中
		record_page_with_same_keywords(exsited_key_pos, *page_iter);


		if(output_count > 0)
		{
			//输出提取结果	
			output_result(*page_iter);

			//输出hotrank 
			if(this->hotrank_handle)
			{
				this->hotrank_handle << (*page_iter)->url << "\t" << output_count << "\n";
			}

			output_count--;
		}
	}


}
//TODO:CESHI  --sort
//计算每个page的最终权值并按照权值由大到小排序
int NewsProcess::calculate_page_weight()
{
	for(vector<struct page_info*>::iterator iter = page_array.begin(); iter != page_array.end(); iter++)
	{
		(*iter)->hot_rank = 1.0; // 初始化hot_rank
		for(int i = 0; i < MAX_KEYWORDS_NUMS; i++)
		{
			if((*iter)->keywords[i] != "")
			{
				string temp_key = (*iter)->keywords[i];
				double key_value = this->key_map[temp_key]->key_value;
				(*iter)->hot_rank *= key_value;
			}
		}
	}
	std::sort(page_array.begin(), page_array.end(), sort_by_hot_rank);	
	return 0;
}


//释放所有动态分配的内存
void NewsProcess::destory_memory()
{
	for(vector<struct page_info*>::iterator iter = this->page_array.begin(); iter !=this->page_array.end(); iter++)
	{
		delete(*iter);
	}
	for(map<string, struct keyword_info*>::iterator iter = this->key_map.begin(); iter != this->key_map.end(); iter++)
	{
		delete(iter->second->pos_in_page_array);
		delete(iter->second);
	}
}

//解析网页的特定属性。 比如 url, site  等
int NewsProcess::parse_page(const string& line, struct page_info *p_page_info)
{
	if("" == line || NULL == p_page_info)
	{
		return -1;
	}
	pcrecpp::RE_Options options;
	options.set_caseless(true);
	options.set_dotall(true);

	vector<string> keys;
	keys.push_back("url");
	keys.push_back("site");
	keys.push_back("pdate");
	keys.push_back("crawtime");
	keys.push_back("tag");
	keys.push_back("newsrank");
	keys.push_back("title");
	keys.push_back("kws");

	for(vector<string>::const_iterator iter = keys.begin(); iter != keys.end(); iter++)
	{
		string regex = "<\\s*" + *iter + "\\s*:\\s*([^>]*)\\s*>" ;
		pcrecpp::RE re(regex, options);
		string result;

		if(re.PartialMatch(line,&result))
		{
			if("url" == (*iter))
			{			
				p_page_info->url = result;
			}
			else if("site" ==(*iter))
			{
				p_page_info->site = result;
			}
			else if("pdate" == (*iter))
			{
				long int integer_result = std::atol(result.c_str());
				if(integer_result != 0)
				{
					p_page_info->pdate = static_cast<time_t>(integer_result);
				} 
			}
			else if("crawtime" == (*iter))
			{
				long int integer_result = std::atol(result.c_str());
				if(integer_result != 0)
				{
					p_page_info->crawltime = static_cast<time_t>(integer_result);
				} 

			}
			else if("tag" == (*iter))
			{
				p_page_info->tag = result;
			}
			else if("newsrank" == (*iter))
			{
				int integer_result = std::atoi(result.c_str());
				if(integer_result != 0)
				{
					p_page_info->newsrank = integer_result;
				} 
			}
			else if("title" == (*iter))
			{
				//将标题中&quot字符替换	
				StringFun::replace_all(result, "&quot;", "\"");
				p_page_info->title = result;
			}
			else if("kws" == (*iter))
			{
				p_page_info->kws = result;
				return extract_keywords(result, p_page_info);

			}

		}		
	}

	return 0;
}

//对同一站点下的相同的文章进行控制， 相同的的文章（kws相同）在同一站点下只记录一次
//如果同一站点有多篇相同文章，则只保留pdate最大的一篇，其他的均忽略
int NewsProcess::update_kws_site_multimap(struct page_info* p_page_info)
{
	string kws = p_page_info->kws;
	typedef multimap<string, struct page_info*>::iterator kws_iter;
	pair<kws_iter, kws_iter> pos = this->kws_site_multimap.equal_range(kws);
	while(pos.first != pos.second)
	{
		//同一site出现多篇相同文章,以pdate最大的为准			    
		if(pos.first->second->site == p_page_info->site)
		{
			#ifndef NDEBUG
			string outline = "filter_info\t";
			outline += this->classification_name + "\t";
			outline += this->file_generated_time_str + "\t"; 
			outline += "same_kws_one_site\t";
			outline += p_page_info->site + "\t" ;
			outline += p_page_info->url + "\t" ;
			outline += p_page_info->title + "\t" ;
			DLOG(INFO) << outline;
			#endif

			if(pos.first->second->pdate < p_page_info->pdate)
			{
				pos.first->second = p_page_info;
			}

			return 0;

		} 
		++pos.first;
	}
	this->kws_site_multimap.insert(pair<string, struct page_info*>(kws, p_page_info));
	return 0;		

}
int NewsProcess::extract_keywords(const string &key_join, struct page_info* p_temp_page)
{
	if(NULL == p_temp_page)
	{
		return -1;
	}

	vector<string> temp_keywords = StringFun::explode("|", key_join);
	vector<string> keywords ;
	for(vector<string>::iterator iter = temp_keywords.begin(); iter != temp_keywords.end(); iter++)
	{
		if(0 == this->black_keywords.count(*iter)) //not black_keywords
		{
			keywords.push_back(*iter);
		}
	}	

	//如果解析出来的keywords小于3个，则将其丢弃

	int keywords_count = keywords.size();
	if(keywords_count < 3)
	{
#ifndef NDEBUG
		string outline = "filter_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 
		outline += "key_amount\t";
		outline += p_temp_page->url + "\t" ;
		outline += p_temp_page->title + "\t" ;

		string key1 = "";
		for(int i = 0; i < keywords_count; i++ )
		{
			key1 += keywords[i] + " " ;
		}
		outline += key1;
		DLOG(INFO) << outline;
#endif
		return -1;
	}


	if(MAX_KEYWORDS_NUMS < keywords_count)
	{
		for(int i = 0; i != MAX_KEYWORDS_NUMS; i++ )
		{
			p_temp_page->keywords[i] = keywords[i];
			p_temp_page->keywords[i] = keywords[i];
			p_temp_page->keywords[i] = keywords[i];
		}
	}
	else
	{
		for(int i = 0; i != keywords_count; i++ )
		{
			p_temp_page->keywords[i] = keywords[i];
			p_temp_page->keywords[i] = keywords[i];
			p_temp_page->keywords[i] = keywords[i];
		}

	}

	return 0;

}

int NewsProcess::start(string file_name)
{

	cout << "file_name: " << file_name << endl;
	// 只解析xml文件	
	if(StringFun::get_file_extension(file_name) != "xml")
	{
		DLOG(ERROR) << file_name + "is not .xml";
		return -1;
	}
	//初始化NewsProcess各种变量中数据
	if(this->init(file_name) != 0)
	{
		return -1;
	}

	std::ifstream in_file;
	string absolute_file_name = INPUT_DIR + "/" + this->file_name;
	in_file.open(absolute_file_name.c_str());
	if(!in_file)
	{
		DLOG(ERROR) <<"can't open: " << absolute_file_name;
		return -1;
	}


	string line;
	unsigned long int pos_in_array = 0;
	while(getline(in_file, line))
	{	

		struct page_info *p_temp_page = new struct page_info();
		if(NULL == p_temp_page)
		{
			DLOG(ERROR) << "new struct page_info error";
			return -1;
		}


		//获得page_info的各种属性
		if(parse_page(line, p_temp_page) != 0)
		{
			//parse_page时且分出的关键词小于指定数目
			continue;
		}

        // black_sites
        if(this->black_sites.count(p_temp_page->site) != 0)
        {
#ifndef NDEBUG
				string outline = "filter_info\t";
				outline += this->classification_name + "\t";
				outline += this->file_generated_time_str + "\t"; 
				outline += "black_site\t";
				outline += p_temp_page->site + "\t" ;
				outline += p_temp_page->url + "\t" ;
				outline += p_temp_page->title  ;
				DLOG(INFO) << outline; 
#endif
            continue;
        }
		initialize_site_rank(p_temp_page);

		initialize_site_factor(p_temp_page);


		//对发帖所属的时间段进行统计 
		if(statistic_section(this->file_generated_time, p_temp_page) != 0)
		{
			continue;
		}

		//给此网页编号
		p_temp_page->pos_in_array = pos_in_array; 
		pos_in_array++;

		//this->page_array.push_back(p_temp_page);

		//确保同一个kws在相同的site下只记录一次
		update_kws_site_multimap(p_temp_page);
	}

	//经过kws-site去重之后的网页放入page_array
	for(multimap<string, struct page_info*>::iterator m_iter = this->kws_site_multimap.begin(); m_iter != this->kws_site_multimap.end(); ++m_iter)
	{	
		this->page_array.push_back(m_iter->second);
	}



	//对关键词信息进行统计
	if(statistics_all_keywords_details() != 0)
	{
		DLOG(ERROR) << "statistics_all_keywords_details error ";
		return -1;
	}

	//计算关键词权值
	calculate_key_weight();

	int key_top_n = 1000 ;// 将前top_n个关键词记录进日志,如果key_top_n < 0 ，则输出所有关键词
	print_key_struct(this->key_map, -1);	

	//计算每个page的最终权值并按照由大到小排序
	calculate_page_weight();

	print_page_array(page_array);


	//输出前output_nums个值到指定的文件夹
	int output_nums = 200;
	output_top_n_news(output_nums);



	//output文件重命名  //client32v上gcc版本较低，TODO:会对以下注释的语句报错，暂且注释掉
	//后续用C语言方法实现
	string temp_outfile_name = this->outfile_name + ".bak";
	//ifstream test_handle(temp_outfile_name.c_str());
	//test_handle.seekg(0, test_handle.end);
	//if(test_handle.tellg() != 0)
	//{
	rename(temp_outfile_name.c_str(), this->outfile_name.c_str());	

	//}
	//test_handle.close();

	//hotrank文件重命名
	string temp_hotrank_filename = this->hotrank_filename + ".bak";
	//test_handle.open(temp_hotrank_filename.c_str());
	//test_handle.seekg(0, test_handle.end);
	//if(test_handle.tellg() != 0)
	//{
	rename(temp_hotrank_filename.c_str(), this->hotrank_filename.c_str());	
	//}

	//将old_path中的.xml 文件移动至new_path（备份）
	string old_path = INPUT_DIR +"/" + this->file_name;
	string new_path = INPUT_BAK_DIR + "/" + this->file_name;
	rename(old_path.c_str(), new_path.c_str());		
	//cout << "start 函数完成" << endl;


	return 0;

}
NewsProcess::~NewsProcess()
{
	//销毁内存
	destory_memory();
	this->outfile_handle.close();
	this->meta_handle.close();
	this->hotrank_handle.close();
	this->newshot_handle.close();


}

void print_page_struct(struct page_info* p_temp_page)
{
	cout << "-----------------------------------------------------"<<endl;
	cout << endl;
	cout << "p_temp_page->pos_in_array = " << p_temp_page->pos_in_array << endl;
	cout << "p_temp_page->url = "	<< p_temp_page->url <<endl;
	cout << "p_temp_page->site = " << p_temp_page->site <<endl;
	cout << "p_temp_page->pdate = " << p_temp_page->pdate <<endl;
	cout << "p_temp_page->section = " << p_temp_page->section <<endl;
	cout <<"p_temp_page->section_weighted_nums = "  << p_temp_page->section_weighted_nums <<endl;
	cout <<"p_temp_page->newsrank = " << p_temp_page->newsrank <<endl;
	cout <<"p_temp_page->site_rank =" << p_temp_page->site_rank <<endl;
	cout <<"p_temp_page->title = " << p_temp_page->title << endl; 
	cout <<"p_temp_page->keywords[0] = " << p_temp_page->keywords[0] << endl;
	cout <<"p_temp_page->keywords[1] = " << p_temp_page->keywords[1] << endl;
	cout <<"p_temp_page->keywords[2] = " << p_temp_page->keywords[2] << endl;
	cout <<"p_temp_page->keywords[3] = " << p_temp_page->keywords[3] << endl;
	cout << endl; 	
	cout << "-----------------------------------------------------"<<endl;

}
//按照初始得的关键词出现个数排序
bool cmp(const pair<string, struct keyword_info*>& x, const pair<string, struct keyword_info*>& y)
{
#ifndef NDEBUG
	return x.second->key_nums_original > y.second->key_nums_original;
#endif
	return x.second->key_value > y.second->key_value;
}
//输出前key_top_n个关键词，如果 key_top_n < 0, 则输出所有关键词
void NewsProcess::print_key_struct(map<string,struct keyword_info*>& key_map,int key_top_n)
{
#ifndef NDEBUG
	typedef pair<string, struct keyword_info*> PAIR;
	vector<PAIR> key_vector;

	for(map<string, struct keyword_info*>::iterator map_iter = key_map.begin(); map_iter != key_map.end(); map_iter++)
	{
		key_vector.push_back(make_pair(map_iter->first, map_iter->second));
	}

	sort(key_vector.begin(), key_vector.end(), cmp); //对关键词进行排序

	int count = key_top_n;


	for(vector<PAIR>::iterator iter = key_vector.begin(); iter != key_vector.end(); iter++)
	{
		count--;
		if(key_top_n > 0 && count < 0)
		{
			break;
		}
		struct keyword_info* p_temp_key = iter->second;


		//cout << iter->first << " " << p_temp_key->section_nums[0] <<"-" << p_temp_key->section_nums[1] <<"-" << p_temp_key->section_nums[2] << "-" << p_temp_key->section_nums[3] << "-" << p_temp_key->section_nums[4] << " " << p_temp_key->key_value <<"  " <<  p_temp_key->key_avg_nums << endl;


		string outline = "keyword_info\t";
		outline += this->classification_name + "\t";
		outline += this->file_generated_time_str + "\t"; 

		string keywords = iter->first;
		outline += keywords + "\t";

		outline += "original: ";
		outline += StringFun::to_string(p_temp_key->section_nums_original[0]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums_original[1]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums_original[2]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums_original[3]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums_original[4]) + " " ;	
		outline +=StringFun::to_string(p_temp_key->key_nums_original) + "\t";

		outline += "weighted: ";
		outline += StringFun::to_string(p_temp_key->section_nums[0]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums[1]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums[2]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums[3]) + "-" ;
		outline += StringFun::to_string(p_temp_key->section_nums[4]) + " " ;	
		outline += StringFun::to_string(p_temp_key->key_value) + "\t";

		DLOG(INFO) << outline;

	}	
#endif
}

void print_page_array(vector<struct page_info*>& page_array)
{
	cout << "-----------------------------------------------------"<<endl;
	cout << "-----------------------------------------------------"<<endl;

	int pos = 0;
	for(vector<struct page_info*>::iterator iter = page_array.begin(); iter != page_array.end(); iter++,pos++)
	{

		cout << endl;
		//	cout << "pos: "<< pos << "  url: " <<  (*iter)->url << "  title: " << (*iter)->title << "  hot_rank: " << (*iter)->hot_rank << endl;
		cout << "pos: " << pos <<endl;
		print_page_struct(*iter);
		cout << endl;
	}
	cout << "-----------------------------------------------------"<<endl;
	cout << "-----------------------------------------------------"<<endl;
}


int main(int argc, char *argv[])
{
	if(argc !=2 )
	{
		cout << "arguments error\n";
		return -1;
	}
	//初始化glog
	google::InitGoogleLogging("news_process");
	google::SetLogDestination(google::INFO,LOG_DIR.c_str());
	google::SetLogDestination(google::ERROR,LOG_DIR.c_str());
	google::InstallFailureSignalHandler();
	NewsProcess news_processer;	
//	string file_name = "social_2013-08-15-09-10-01.xml";
	news_processer.start(argv[1]);
//	news_processer.start(file_name);
	return 0 ;
}

